{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.12 64-bit ('tf1')",
   "display_name": "Python 3.6.12 64-bit ('tf1')",
   "metadata": {
    "interpreter": {
     "hash": "b61dba104367621e4544265f53912e0eee229b600776d472b4c852dafdface24"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "from imageai.Detection import ObjectDetection\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten, UpSampling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import random\n",
    "import glob\n",
    "# import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "import subprocess\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow, figure\n",
    "\n",
    "from keras.layers import Lambda, Reshape, Permute, Input, add, Conv3D, GaussianNoise, concatenate\n",
    "from keras.layers import ConvLSTM2D, BatchNormalization, TimeDistributed, Add\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/pratt3000/Documents/projects/Violence Detection-v3/models/yolo-tiny.h5\n"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "detector = ObjectDetection()\n",
    "detector.setModelTypeAsTinyYOLOv3() \n",
    "detector.setModelPath(config.PRETRAINED_MODEL_PATH)\n",
    "detector.loadModel(detection_speed = config.OBJECT_DETECTION_SPEED) #change parameter to adjust accuracy and speed\n",
    "custom = detector.CustomObjects(person=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"num_epochs\": 10, \n",
    "    \"batch_size\": 32,\n",
    "    \"FONT\" : cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    \"IMG_SIZE\" : 64,\n",
    "    \"FRAME_BATCH_SIZE\" : 4,\n",
    "    \"THRESHOLD_DIFF_TOLERANCE\" : 35  \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(config=hyperparams)\n",
    "# config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_generater(batch_size):     \n",
    "  ########### YET TO ADD VALIDATION/TEST/OTHER DATASETS ############\n",
    "    list_fight=os.listdir(\"/home/pratt3000/Documents/projects/Violence Detection-v3/movies/Violence\")\n",
    "    list_no_fight=os.listdir(\"/home/pratt3000/Documents/projects/Violence Detection-v3/movies/NonViolence\")\n",
    "\n",
    "    fight_final=random.sample(list_fight, 800)\n",
    "    no_fight_final=random.sample(list_no_fight,800)\n",
    "\n",
    "    fight_labels = []\n",
    "    no_fight_labels = []\n",
    "\n",
    "    for i in range (800):\n",
    "        fight_labels.append([1,0])\n",
    "        no_fight_labels.append([0,1])\n",
    "\n",
    "    final = fight_final + no_fight_final\n",
    "    labels = fight_labels + no_fight_labels\n",
    "\n",
    "    c = list(zip(final,labels))\n",
    "    random.shuffle(c)\n",
    "    names, labels = zip(*c)\n",
    "\n",
    "    images_batches=[]\n",
    "    labelsss=[]\n",
    "\n",
    "    for i in range(len(names)): ##no. of videos loop\n",
    "       \n",
    "        if labels[i]==[0,1]:\n",
    "            in_dir= \"/home/pratt3000/Documents/projects/Violence Detection-v3/movies/NonViolence\"\n",
    "        else:\n",
    "            in_dir= \"/home/pratt3000/Documents/projects/Violence Detection-v3/movies/Violence\"\n",
    "        in_file = os.path.join(in_dir, names[i])\n",
    "        vidcap = cv2.VideoCapture(in_file)\n",
    "        length = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        for j in range(int(length/config.FRAME_BATCH_SIZE)):\n",
    "            detections_temp_2=[]\n",
    "            success,frame_temp = vidcap.read()\n",
    "            frame_temp = cv2.cvtColor(frame_temp, cv2.COLOR_BGR2GRAY)\n",
    "            detections_temp = get_boxes(frame_temp) \n",
    "            frame_temp = mask_frame(frame, detection_temp, detections_temp_2)                    \n",
    "            images_frame_batches=[]\n",
    "           \n",
    "            for k in range(config.FRAME_BATCH_SIZE - 1):\n",
    "               \n",
    "                success,frame = vidcap.read()\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)   \n",
    "                detections = get_boxes(frame)                   \n",
    "                frame = mask_frame(frame, detections, detections_temp)          \n",
    "                diff = get_frame_difference(frame, frame_temp)\n",
    "                images_frame_batches.append(diff)\n",
    "                frame_temp = frame\n",
    "                detections_temp=detections\n",
    "           \n",
    "            images_batches.append(images_frame_batches)\n",
    "        \n",
    "        aux = np.ones([int(length/config.FRAME_BATCH_SIZE),2])\n",
    "        labelss = labels[i]*aux\n",
    "        labelss = labelss.tolist()\n",
    "        lebelsss.append(labelss)\n",
    "    \n",
    "    labelsss_final=np.reshape(labelsss,(len(names)*int(length/config.FRAME_BATCH_SIZE),2))\n",
    "    counter = 0\n",
    "    input_images=[]\n",
    "    output_labels\n",
    "    while True:\n",
    "        input_imgs=np.zeros(\n",
    "            (batch_size,config.FRAME_BATCH_SIZE, config.IMG_SIZE, config.IMG_SIZE, 1))\n",
    "        if (counter+batch_size >= len(labelsss)):\n",
    "            counter = 0\n",
    "       \n",
    "        for i in range(batch_size):\n",
    "            input_imgs = images_batches[counter + i]\n",
    "            input_images.append(input_imgs)\n",
    "            output_label = labelsss_final[counter + i]\n",
    "            output_labels.append(output_label)\n",
    "            input_imgs[i] /= 255.\n",
    "            \n",
    "        yield (input_images, output_labels)\n",
    "        counter += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes(frame):\n",
    "    _, detections = detector.detectCustomObjectsFromImage(\n",
    "        custom_objects=custom,\n",
    "        input_type=\"array\",\n",
    "        input_image= frame,\n",
    "        output_type=\"array\"\n",
    "        )\n",
    "    return detections\n",
    "\n",
    "def frame_preprocessing(frame):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame = cv2.resize(frame, dsize=(config.IMG_SIZE, config.IMG_SIZE), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def get_frame_difference(frame_1, frame_2):\n",
    "    frame = cv2.absdiff(frame_1, frame_2)\n",
    "    ret,thresh1 = cv2.threshold(frame,config.THRESHOLD_DIFF_TOLERANCE,255,cv2.THRESH_BINARY)   #127 is threshold\n",
    "    return thresh1\n",
    "\n",
    "def mask_frame(frame, detections, detections_temp):\n",
    "    mask = np.zeros(frame.shape, dtype=np.uint8)\n",
    "\n",
    "    for eachObject in detections:\n",
    "        x1,y1,x2,y2 = eachObject[\"box_points\"]\n",
    "        mask = cv2.rectangle(mask, (x1, y1),(x2,y2), (255,255,255), -1) \n",
    "\n",
    "    for eachObject in detections_temp:\n",
    "        x1,y1,x2,y2 = eachObject[\"box_points\"]\n",
    "        mask = cv2.rectangle(mask, (x1, y1),(x2,y2), (255,255,255), -1) \n",
    "\n",
    "    result = cv2.bitwise_and(frame, mask)   # Mask input image with binary mask\n",
    "    result[mask==0] = 255   # Optional : set background -> now white/ by default black\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_8 (InputLayer)            (None, 3, 512, 512,  0                                            \n__________________________________________________________________________________________________\npermute_3 (Permute)             (None, 512, 512, 1,  0           input_8[0][0]                    \n__________________________________________________________________________________________________\ngaussian_noise_2 (GaussianNoise (None, 512, 512, 1,  0           permute_3[0][0]                  \n__________________________________________________________________________________________________\npermute_4 (Permute)             (None, 3, 512, 512,  0           gaussian_noise_2[0][0]           \n__________________________________________________________________________________________________\nconv_lstm1 (ConvLSTM2D)         (None, 3, 512, 512,  736         permute_4[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_39 (BatchNo (None, 3, 512, 512,  16          conv_lstm1[0][0]                 \n__________________________________________________________________________________________________\ntime_distributed_5 (TimeDistrib (None, 3, 256, 256,  0           batch_normalization_39[0][0]     \n__________________________________________________________________________________________________\nconv_lstm2 (ConvLSTM2D)         (None, 3, 256, 256,  3488        time_distributed_5[0][0]         \n__________________________________________________________________________________________________\nbatch_normalization_40 (BatchNo (None, 3, 256, 256,  32          conv_lstm2[0][0]                 \n__________________________________________________________________________________________________\ntime_distributed_6 (TimeDistrib (None, 3, 128, 128,  0           batch_normalization_40[0][0]     \n__________________________________________________________________________________________________\nconv_lstm3 (ConvLSTM2D)         (None, 3, 128, 128,  13888       time_distributed_6[0][0]         \n__________________________________________________________________________________________________\ntime_distributed_7 (TimeDistrib (None, 3, 256, 256,  0           conv_lstm3[0][0]                 \n__________________________________________________________________________________________________\nconv_lstm4 (ConvLSTM2D)         (None, 3, 256, 256,  18496       time_distributed_7[0][0]         \n__________________________________________________________________________________________________\nbatch_normalization_41 (BatchNo (None, 3, 256, 256,  64          conv_lstm4[0][0]                 \n__________________________________________________________________________________________________\nconv_lstm5 (ConvLSTM2D)         (None, 3, 256, 256,  6944        batch_normalization_41[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_42 (BatchNo (None, 3, 256, 256,  32          conv_lstm5[0][0]                 \n__________________________________________________________________________________________________\nadd_2 (Add)                     (None, 3, 256, 256,  0           batch_normalization_40[0][0]     \n                                                                 batch_normalization_42[0][0]     \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 3, 256, 256,  0           add_2[0][0]                      \n__________________________________________________________________________________________________\ntime_distributed_8 (TimeDistrib (None, 3, 512, 512,  0           dropout_6[0][0]                  \n__________________________________________________________________________________________________\nconv_lstm6 (ConvLSTM2D)         (None, 512, 512, 4)  1744        time_distributed_8[0][0]         \n__________________________________________________________________________________________________\nbatch_normalization_43 (BatchNo (None, 512, 512, 4)  16          conv_lstm6[0][0]                 \n__________________________________________________________________________________________________\nflatten_2 (Flatten)             (None, 1048576)      0           batch_normalization_43[0][0]     \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 50)           52428850    flatten_2[0][0]                  \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 2)            102         dense_3[0][0]                    \n==================================================================================================\nTotal params: 52,474,408\nTrainable params: 52,474,328\nNon-trainable params: 80\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "inp = Input((config.FRAME_BATCH_SIZE - 1,config.IMG_SIZE,config.IMG_SIZE,1))\n",
    "permuted = Permute((2,3,4,1))(inp)\n",
    "noise = GaussianNoise(0.1)(permuted)\n",
    "c=4\n",
    "x = Permute((4,1,2,3))(noise)\n",
    "x =(ConvLSTM2D(filters=c, kernel_size=(3,3),padding='same',name='conv_lstm1', return_sequences=True))(x)\n",
    "\n",
    "c1=(BatchNormalization())(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x =(TimeDistributed(MaxPooling2D(pool_size=(2,2))))(c1)\n",
    "\n",
    "x =(ConvLSTM2D(filters=2*c,kernel_size=(3,3),padding='same',name='conv_lstm2',return_sequences=True))(x)\n",
    "c2=(BatchNormalization())(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x =(TimeDistributed(MaxPooling2D(pool_size=(2,2))))(c2)\n",
    "x =(ConvLSTM2D(filters=4*c,kernel_size=(3,3),padding='same',name='conv_lstm3',return_sequences=True))(x)\n",
    "\n",
    "x =(TimeDistributed(UpSampling2D(size=(2, 2))))(x)\n",
    "x =(ConvLSTM2D(filters=4*c,kernel_size=(3,3),padding='same',name='conv_lstm4',return_sequences=True))(x)\n",
    "x =(BatchNormalization())(x)\n",
    "\n",
    "x =(ConvLSTM2D(filters=2*c,kernel_size=(3,3),padding='same',name='conv_lstm5',return_sequences=True))(x)\n",
    "x =(BatchNormalization())(x)\n",
    "x = Add()([c2, x])\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x =(TimeDistributed(UpSampling2D(size=(2, 2))))(x)\n",
    "x =(ConvLSTM2D(filters=c,kernel_size=(3,3),padding='same',name='conv_lstm6',return_sequences=False))(x)\n",
    "x =(BatchNormalization())(x)\n",
    "\n",
    "x = (Flatten())(x)\n",
    "x = (Dense(units=50, activation='relu'))(x)\n",
    "\n",
    "x = (Dense(units=2, activation='relu'))(x)\n",
    "\n",
    "model=Model(inputs=[inp], outputs=[x])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name &#39;train_dir&#39; is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m&lt;ipython-input-42-7eadcb8f8ffa&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit_generator(\n\u001b[0;32m----&gt; 4\u001b[0;31m                     \u001b[0mmy_generater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name &#39;train_dir&#39; is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "train\n",
    "model.fit_generator(\n",
    "                    my_generater(4, train_dir),\n",
    "                    steps_per_epoch=steps_per_epoch//4,\n",
    "                    epochs=10, \n",
    "                    callbacks=[ImageCallback(), WandbCallback()],\n",
    "                    validation_steps=validation_steps//4,\n",
    "                    validation_data=my_generator(4, val_dir)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        validation_X, validation_y = next(\n",
    "            my_generator(15, val_dir))\n",
    "        output = self.model.predict(validation_X)\n",
    "        wandb.log({\n",
    "            \"input\": [wandb.Image(np.concatenate(np.split(c, 5, axis=2), axis=1)) for c in validation_X],\n",
    "            \"output\": [wandb.Image(np.concatenate([validation_y[i], o], axis=1)) for i, o in enumerate(output)]\n",
    "        }, commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "error",
     "evalue": "OpenCV(4.4.0) /tmp/pip-req-build-6amqbhlx/opencv/modules/imgproc/src/resize.cpp:3929: error: (-215:Assertion failed) !ssize.empty() in function &#39;resize&#39;\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m&lt;ipython-input-11-90ed7dcd6e64&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---&gt; 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/Violence Detection-v3/src/main.py\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvidcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# get frame serially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--&gt; 113\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m                   \u001b[0;31m# get obj detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.4.0) /tmp/pip-req-build-6amqbhlx/opencv/modules/imgproc/src/resize.cpp:3929: error: (-215:Assertion failed) !ssize.empty() in function &#39;resize&#39;\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from keras.layers import Lambda, Reshape, Permute, Input, add, Conv3D, GaussianNoise, concatenate\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten, UpSampling2D\n",
    "from keras.layers import ConvLSTM2D, BatchNormalization, TimeDistributed, Add\n",
    "from keras.models import Model\n",
    "import config\n",
    "\n",
    "from imageai.Detection import ObjectDetection\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import main \n",
    "\n",
    "\n",
    "model = main.get_model()\n",
    "#model.load_weights(\"/home/pratt3000/Documents/projects/Violence Detection-v3/models/classification/saved-model-01-0.56.hdf5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}